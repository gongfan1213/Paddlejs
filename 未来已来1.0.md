### 第12章 未来已来
2020年6月19日，特斯拉前AI主管Andrej Karpathy在“把玩”了OpenAI的GPT-3一个小时后，在Twitter（推特）上与OpenAI的团队负责人Chris Olah沟通时，明确了软件开发已经进入新的3.0阶段，即Prompt（提示）设计阶段，如图12-1所示。

事实上，GPT模型在处理Zero-Shot问题时展现出了超强的能力，即在不用给出任务样例，只给出任务描述和任务提示的前提下就能产出丰富的答案。

说明：零次学习（Zero-Shot Learning，ZSL）期望模型能够像成熟的人类大脑一样，对从未见过的事物进行分类，具有推理能力，实现真正的智能。除了Zero-Shot，还有One-Shot和Few-Shot，它们的定义如下。

- Zero-Shot：只需要给出任务描述（Description）和任务提示（Prompt），模型就能生成答案。

- One-Shot：在给出任务描述的基础上，再给出一个例子（Example），并给出任务提示，模型就能生成答案。 

- Few-Shot：与One-Shot的区别在于，需要给出更多的例子。

细心的读者会发现，这里不断出现提示（Prompt）一词，这也是GPT模型与人类交互的主要接口。不妨看一下，OpenAI为了开发者能够快速体验其API设计了Playground工具，如图12-2所示。虽然图12-2的右侧面板包含了丰富的API组件（如Mode、Model、Temperature等）以影响最终模型产生的内容，但主要的功能还是与提示有关。
程序员（或其他职业从业人员）在调用OpenAI API的过程中，主要工作从设计算法和管理数据，变成了如何设计精巧的提示，从而让模型按照需求完成任务。

关于ChatGPT或文心一言等大语言模型（Large Language Model，LLM）如何出色地完成人类指定的任务的示例，这里不再一一赘述。在本书付梓之际，相信绝大多数读到本书的读者都或多或少地体会到了大语言模型带来的惊艳效果。我们不妨先了解一下以GPT为代表的大语言模型的基本运行原理。

#### 12.1 大语言模型简介

语言模型（Language Model）是自然语言处理应用程序中的重要组成部分。它的应用范围很广，文本生成、机器翻译和语音识别等领域都需要用到语言模型。在自然语言处理中，语言模型是一个非常重要的概念，因为它可以帮助机器更好地理解人类的语言。语言模型能够从大量的语料库中学习单词和句子的结构，并预测下一个单词或句子的概率分布，从而生成更加自然的文本。例如，在手机上使用自动完成功能时，当键入“生日”时，自动完成可能会提供“快乐”或“蛋糕”等建议。语言模型背后的算法基于概率模型来预测下一个单词或句子。

在GPT-3之前，自然语言处理任务的表现能力十分有限，没有一种通用的语言模型可以达到良好的生成文本的效果，一方面是因为技术路线（BERT或GPT）的选择，另一方面是因为在语料库和参数量没有达到一定数量级时，无法让语言模型发挥出“神奇”的效果。


提示：BERT（Bidirectional Encoder Representations from Transformers）和GPT（Generative Pre-trained Transformer）都是基于Transformer架构的自然语言处理（Natural Language Processing，NLP）模型，BERT主要用于自然语言理解（Natural Language Understanding，NLU）任务，如问答、情感分析和实体识别等，GPT主要用于自然语言生成（Natural Language Generation，NLG）任务，如文本生成、文本摘要和对话生成等。对于二者的训练方式之间的区别，形象地来说：BERT是在做“完形填空”，GPT是在做“文本续写”。

**12.1.1 什么是GPT**

在深入了解GPT之前，有必要了解“GPT”这三个字母的由来。

1. **G——Generative Models**

GPT是一种生成模型，因为它可以生成文本，目前是自然语言处理领域中最先进的技术之一，它可以应用于各种应用程序，如聊天机器人、文本摘要和翻译等。

虽然我们日常都要面对大量的信息，但是需要明确的是，信息本身并没有价值。真正的价值在于我们如何利用这些信息。因此，开发能够分析和理解数据宝库的智能模型和算法至关重要。

生成模型就是其中之一，它可以通过分析现有数据的规律，从而生成新的数据。这种方法不仅可以帮助我们更好地理解数据，还可以提供更多的信息，从而促进我们深入探索研究的领域。

训练模型是机器学习的关键步骤之一，它需要准备和预处理数据集。数据集是一组示例，可以帮助模型学习执行给定的任务。通常来说，数据集是某个特定领域的大量数据，如某些数据集有数百万张汽车图像，人们可以用它“教”模型什么是汽车。数据集也可以是句子或音频样本，用来训练模型生成类似的数据。这些数据集可以从许多渠道获得，包括互联网、数据库和其他公共资源。

在准备数据集之前，首先要确定模型需要什么类型的数据，这意味着需要了解模型的输入和输出，然后准备数据集，通常包括数据清洗、特征提取和数据转换等步骤。这些步骤可以确保数据集是干净和可用的，可以有效地用于训练模型。

在准备好数据集后，就可以开始训练模型了。训练模型是一个计算密集的过程，涉及迭代和多步优化。在训练过程中，模型将不断地调整自己的权重和偏差，以最小化损失函数并提高准确性。一旦完成了训练，就可以测试模型，并根据需要进行微调和优化。

2. **P——Pre-Trained Models**

预训练模型（Pre-Trained Models，PTMs）把NLP的发展带入了一个新的时代。为了创建一个表现良好的模型，需要使用一组称为参数的特定的变量进行训练，其间需要不断地迭代，深度学习模型需要很长时间才能找到这些理想的参数。这是一个漫长的过程，取决于任务的不同，可能需要数小时到数月的时间和大量的算力。这时，预训练模型就发挥了重要的作用，它与具体的任务无关，作为基座被迁移至各种具体任务中以提升整体的训练效率。

预训练模型是一种为更一般的任务进行训练的模型，可以使用类似迁移学习的方式，将预训练模型作为起点，并附加训练数据集来进行微调。这样，可以获得适用于特定问题的模型，而不必从头开始构建模型。

但是，表现良好的预训练模型是建立在大规模数据集的基础之上的，GPT依赖的文本语料库中的数据量非常庞大，且来源丰富，包括网页抓取数据、图书文献及维基百科等。

我们经常听到有人说GPT模型的演进是一个“大力出奇迹”的过程，就在于这些大量且多样化的文本语料库。GPT在完成指定任务的过程中不需要用户提供任何额外的示例数据，给人一种具备“创造力”的使用体验。

3. **T——Transformer Models**

Transformer Models是一种机器学习模型，可以一次性处理文本序列，而不是逐个处理单词，并且具有强大的理解单词之间关系的能力。

自2017年被推出以来，Transformer Models已成为学术界和工业界应对各种自然语言处理任务的事实标准，并且衍生出了很多应用程序。对程序员来说，最熟悉的无疑是GitHub的Copilot，它可以将注释转换为源代码，并且在必要时帮助程序员快速地完成功能模块的编写。

Transformer Models并非一夜走红，它结合了注意力（Attention）、迁移学习（Transfer Learning）和增强神经网络（Scaling up Neural Networks）等关键思想和技术点。

Transformer Models的基础是序列到序列（Seq2Seq）模型的一种改进。Seq2Seq模型是一种基于“编码器 - 解码器”（Encoder-Decoder）架构的模型（见图12-3），通常使用递归神经网络（RNN）作为编码器和解码器的组件，特别适用于翻译任务。谷歌翻译在2016年便开始使用Seq2Seq模型。

![image](https://github.com/user-attachments/assets/cfb10b36-22ea-4e61-b35d-c486cf770c20)


在使用RNN处理序列数据的Seq2Seq模型中，由于RNN模型存在缺陷，因此在处理较长序列时会遇到性能瓶颈。为了解决这个问题，Transformer Models使用了自注意力（Self-Attention）机制和多头注意力（Multi-Head Attention）机制，取代了传统的RNN结构，从而可以更好地处理长序列，并且提高了模型的并行度，加快了训练速度。

**12.1.2 超大语言模型带来的能力跃升**


语言模型是一种机器学习模型，通常用于预测给定一段文本序列的下一个可能单词或字符。它是一种无监督学习模型，因此不需要人工标记的训练数据。而大语言模型是指通过对大量文本数据进行分析来学习语言的结构和规律，从而生成一个能够预测下一个单词或字符的概率分布。大语言模型可被应用于许多自然语言处理任务，如语音识别、机器翻译、文本生成和自动摘要等。
大语言模型之所以被称为“大”，是因为在预训练阶段使用了庞大的语料库，模型参数量也非常大，如下表所示。


|模型|语料库大小|参数量/个|
| ---- | ---- | ---- |
|GPT-1|40 GB|117M（百万）|
|GPT-2|1.5 TB|1.5B（十亿）|
|GPT-3|570 GB|175B（千亿）|
|GPT-4|未透露，远大于GPT-3|100T（百万亿）|

事实上，当模型的规模达到GPT-3及以上时才能称为超大规模语言模型。而在GPT-3出现之后，行业内针对它新提供的功能，经常用到“涌现”二字。因为这些功能是通过在大量文本数据上进行无监督训练而“自然产生”的。换句话说，GPT-3会执行各种任务，是因为它在训练过程中学会了理解和生成人类语言，并在此基础上推断出如何解决特定问题。
“涌现”描述了一种自组织现象，即在没有明确指导的情况下，在简单的基本规则中自然地产生复杂的行为和功能。在GPT-3中，通过在大规模数据集上进行训练，模型可以生成连贯的文本、回答问题、编写代码，甚至进行一定程度的推理。这些功能并不是针对特定任务进行优化的，而是在训练过程中从数据中自然产生的。
GPT-3的“涌现”现象让人们对AI产生了更高的期望，因为它表明了在训练过程中，深度学习模型可以发现潜在的结构和规律，从而在各种任务中展现出惊人的能力。

**12.1.3 GPT-4的又一次生长**
GPT-4是OpenAI在深度学习扩展方面的里程碑，与之前的版本不同，它是一个大型多模态模型。
多模态是指人和计算机通过除文字外的其他媒介方式交互，如图像、语音和视频等。
虽然在非正式对话中，OpenAI GPT-3.5和GPT-4之间的区别可能不十分明显，但当任务的复杂性达到足够的级别时，区别就会显现出来——GPT-4比GPT-3.5更可靠、更有创意，能够处理比GPT-3.5更复杂的指令。例如，GPT-4在各种专业和学术基准测试中达到了优秀人类的水平，在模拟的律师考试中得分在测试者的前10%，而此前的GPT-3.5的得分则在测试者的后10% 。
除此之外，GPT-4接收的提示语可以包括图像、文字两种媒介类型，它可以针对图像和文字混排的提示语给出相应的文本回答。提示语可以包括带有文本和图像的文档、图表或屏幕截图，GPT-4会给出与纯文本输入时一样的输出。

**12.1.4 回答准确性和可解释性**
尽管GPT-4又“涌现”出许多新的功能，但它与早期的GPT模型具有类似的局限性，即无法保证回答的准确性，用在涉及重要决策的领域（如医疗、审查）中会导致灾难性的结果。与准确性相伴的问题在于模型的“可解释性差”。
GPT-3之后的大语言模型与先前的模型的区别不仅在于能力的跃升，而且在于模型优化和训练流程方面有重大变化。在传统模型的训练过程中，要想在特定领域做出较好的效果，有 
