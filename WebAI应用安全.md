### 第10章 Web AI应用安全

随着移动设备算力的不断增强，随着模型的不断优化及前端推理引擎的不断发展，端AI应用越来越广泛。Web AI作为端AI的一种，在人脸身份认证、穿戴特效、虚拟形象等场景下的应用越来越多。而在Web AI产品商业化的过程中，模型及运行时安全问题变得尤为突出。本章以机密性、完整性和可用性为目标，通过防盗用、防篡改、反编译等安全手段，系统性地探索Web AI推理及应用中的安全问题解决方案。

#### 10.1 安全问题与安全目标

本章以一个具体的案例为切入点，分析在一个完整的Web AI应用运行的全流程中有哪些地方存在安全隐患，以及保护Web AI应用安全的目标是什么。

##### 10.1.1 安全问题

CV模型的应用比较广泛，此处以常见的人脸检测为例，实时获取摄像头的数据在Web页面中绘制出标识人脸位置的方框，达到方框跟随人脸位置移动的效果，如图10-1所示。

**图10-1 人脸检测（图像由GAN生成）**

要实现这个功能，简化步骤如下：

1. 获取用户输入：打开摄像头，从视频流中获取一张图像数据。

2. 前处理：图像裁剪，适配推理模型的输入格式。

3. 推理：在Web环境中完成模型的推理计算，计算出人脸方框的位置坐标。

4. 后处理：将方框坐标映射回原图像。

5. 绘制：在这张图像上绘制出映射后的位置方框。

其中，第2步前处理和第4步后处理都属于对数据的处理，可归为数据前后处理环节。第5步会根据应用场景的需要开发不同的功能，如与人脸相关的渲染特效，可归为应用的实现环节。

一个完备的商业化产品，其技术壁垒在于模型推理的准确性，在Web环境中的具体流程如下。

1. **推理前的准备**：


推理前，要根据模型复杂度、终端性能，以及应用对推理性能、运行环境与设备兼容性的要求，选定一种计算方案，如paddle.js前端推理引擎可选的计算方案有WebGL、WebGPU、WebAssembly、PlainJS和NodeGL等。在Web领域，要保障运行环境的安全性，可以选择WebAssembly计算方案，让推理过程在WebAssembly的沙箱环境中执行，对Web宿主环境暴露有限的调用入口，将整个推理过程黑盒化。

4. **推理过程**：



推理过程分为初始化和推理两大阶段。通常来说，模型提供者与应用提供者可能不是同一个角色。

对于模型提供者来说，模型文件的安全是重要的，即模型的网络结构与权重数据不能被泄漏。


对于这点而言，除了网络加载过程能够直接暴露模型文件，推理过程中的神经网络拓扑结构初始化与算子的执行过程也可能暴露模型的结构信息；对于应用提供者来说，该应用的盈利模式可能按流量计费，所以要保证应用本身不被盗用，包括整个应用运行环境的盗用与关键业务代码的盗用。关键业务代码包括对模型前后数据的处理，以及根据推理结果完成的酷炫渲染特效的实现。

所以，在Web AI应用的整个流程中，推理前的模型文件获取、整个推理过程和推理后的数据处理、逻辑与应用的实现环节均需要被保护。

##### 10.1.2 安全目标


对Web AI应用进行安全问题分析后便可知，为了保障Web AI应用的安全，既要使模型安全，即保护模型的拓扑结构及权重数据，又要使应用安全，即保护应用的运行时，运行时包括推理过程和应用的关键业务代码的实现。


1. **模型安全**：


模型提供者（后面简称为资源方）通常会根据模型拓扑结构、权重数据等模型信息的机密性和模型在使用与传播中的可控性来评估模型的安全强度。要达到这一目标，通常要实现模型内容加密、解密的密钥安全存储和推理运行时受控这个安全铁三角，如图10-2所示。


![image](https://github.com/user-attachments/assets/048a8925-9726-4b2f-9593-a091ed22eba2)


**图10-2 安全铁三角**
 - 模型内容加密
 
 - 解密的密钥安全存储
 
 - 推理运行时受控

2. **应用安全**：


Web AI应用提供者（后面简称为应用方）要保护整个应用环境、关键业务代码不被盗用与篡改。要实现这一目标，需要对应用本身进行安全加固，可通过运行时沙箱隔离、身份认证、运行环境检查、增加反调试策略、代码混淆加密等加固手段，实现防盗用、防篡改、反编译的目标。

因而，Web AI应用的安全目标是保障模型安全和应用安全，使整个Web AI应用具有机密性、完整性和可控性。

#### 10.2 前端安全技术


为了保障模型拓扑结构、权重数据等核心信息的机密性，模型内容在传输前要进行加密；为了保证应用的运行时安全，要让模型内容的解密及推理过程运行在黑盒中，保障代码安全；为了让运行时可控，要在代码安全的基础上进行安全加固。

##### 10.2.1 加解密方案

对于加密，常用的方案有对称加密、非对称加密和哈希算法。通用的对称加密有DES、3DES和AES。目前也有很多轻量分组密码研究，如在边缘计算环境中使用OpenCL和WebAssembly高效实现NIST LWC ESTATE算法进行安全通信。综合加密强度、解密性能等因素，这里给出两种由前端与云端配合完成的加解密方案供参考。

1. **混合密码体制**：混合密码体制是AES对称加密、RSA非对称加密和MAC哈希加密的组合。

由于模型体积在常规的网络静态资源中相对较大，兼顾模型内容的机密性和Web环境中的解密速度，推荐采用密钥长度为256位的AES对称加密算法对模型内容进行加密；为了加大安全强度，可采用RSA非对称加密算法对密钥进一步加密。

对模型内容和密钥加密是为了保障模型的机密性，还可以通过MAC哈希加密算法进一步保障模型的完整性，并验证模型资源请求者的身份。例如，采用HMAC（Keyed-Hashing for Message Authentication）算法对模型内容进行HMAC运算，完成认证加密，得到认证标识。

根据顺序不同，认证加密有三种方案：MAC-and-encrypt、MAC-then-encrypt和encrypt-then-MAC。

用混合密码体制加密后，网络传输的内容包含模型内容密文、密钥密文及认证标识。图10-3描述了混合密码体制与多层密钥体系相结合的场景。

![image](https://github.com/user-attachments/assets/1b4a3d7e-17d9-47ff-997c-29d5030cd2b1)


**图10-3 混合密码体制与多层密钥体系相结合**
 - AES算法用于加密模型内容，所用密钥称为key1。
 - key1用RSA算法加密，对应的加密密钥为key2；用key2加密后的key1密文为key；key的解密密钥为key3。
 - 认证标识将key作为密钥，对模型内容密文进行HMAC运算，得到认证标识code。

2. **轻量的认证加密**：混合密码体制能够保证模型的机密性和完整性，而在Web环境下解密耗时占在线推理运行总耗时的95%，因而密码算法的选择要兼顾安全性和在线解密的性能。轻量级密码算法在降低一点安全强度的前提下，具有吞吐量低、安全级别适中和性能较高的特点。对于轻量级密码的标准化，ISO在2012 - 2019年发布了一系列轻量级密码算法国际标准，美国国家标准与技术研究院（NIST）在2015年启动了轻量级密码算法标准化项目，并在2018年8月开始征集轻量级密码算法并对其进行标准化。本书采用边缘计算环境中使用的结合了OpenCL和WebAssembly的NIST LWC ESTATE算法。

ESTATE采用MAC-then-encrypt的认证加密方式，是轻量级加密算法的第二轮候选算法之一，是一种基于可调整分组密码的认证加密方案，它采用类似FCBC的认证方式进行OFB加密。

##### 10.2.2 代码安全


众所周知，在Web前端领域，业务逻辑开发时用到的HTML、CSS、JavaScript都是可以被直接看到的，因而是不可信的。即使通过对称、非对称、散列等加密技术进行加密，解密的过程仍然可以被劫持和调试，因而保护前端代码安全是必要的。前端代码安全可通过降低代码可读性与代码运行时环境沙箱化来进行，常用的方法如下。


1. **代码混淆**：

代码混淆是常用的降低代码可读性的方法，也是一种常见的抗逆向分析方法，实现原理是通过正则替换或通过语法树替换混淆代码原文。常见的混淆手段如下。
 - **变量名混淆和常量提取**：将变量名替换成难以辨认和理解的字符串，如十六进制变量名。或者把常量存储到数组中，使用时按照数组下标进行索引。
 - **运算混淆**：把逻辑运算、二元运算、字符串拼接等运算封装成函数进行混淆。
 - **语法替换与动态执行**：把常用的语法替换成不常用的语法，如把for替换成do/while。还可以在静态的逻辑代码中添加动态的判断条件来干扰静态代码分析。
 - **控制流平展混淆**：把执行流程和判定流程进行混淆，让攻击者不容易摸清楚代码的执行逻辑。

前端工程项目中常用的UglifyJS实现的是变量名混淆，使用时可指定需要混淆的变量名，也可保留指定变量名不被混淆，不同的构建工具如Webpack、Gulp.js都有对应的插件版本。


2. **WebAssembly**：

代码混淆是一种静态防御手段，会增加反编译的难度，但仍可调试，而WebAssembly是一个可移植、体积小、加载快且兼容Web的全新二进制格式，运行在沙箱环境中，提升了代码的安全性。又因其逼近本机性能的执行效率，在计算密集型领域，如模型推理、游戏、音视频处理等场景下的应用越来越多。

在前端安全加固方面，WebAssembly也有很多应用，如在WebAssembly中实现IBC认证加密方案。WebAssembly是由主流浏览器厂商组成的W3C社区团体制定的一个新规范，拥有高性能、安全、开放和标准四大特性。
 - **高性能**：WebAssembly有一套完整的语义，是体积小且加载快的全新二进制格式，其目标就是充分发挥硬件能力以达到原生执行效率。
 - **安全**：WebAssembly运行在一个沙箱环境中，甚至可以在现有的JavaScript虚拟机中实现。在Web环境中，会严格遵守同源策略和浏览器安全策略。
 - **开放**：WebAssembly设计了一个非常规整的文本格式用来调试、测试、实验、优化、学习、教学或编写程序。可以通过这种文本格式在Web页面上查看模块的源码。
 - **标准**：WebAssembly在Web中被设计成无版本的，特性可测试，向后兼容，可以被JavaScript调用，进入JavaScript上下文也可以像Web API一样调用浏览器的功能，既可以运行在浏览器上，也可以运行在非Web环境下。

##### 10.2.3 安全加固方案

加解密方案保障了模型内容的机密性和完整性，代码安全保障了解密过程及推理过程的安全，但还需要用额外的安全加固方式来进行用户身份的认证和宿主环境的安全保障。借鉴移动终端的常用方案，这里介绍以下加固方案。
1. **身份认证与宿主校验**：

   采用混合密码体制中介绍的加解密方案，会先在推理运行时请求到模型内容的密文、认证码及密钥密文，再对模型内容解密。而在解密之前，要验证宿主环境的安全性及用户身份，如采用IBC认证加密方案。在认证过程中会有一个安全中心作为资源方和应用方的连接，其中待加密的明文信息是加解密方案中产生的密钥密文。
 
 - 安全中心会给资源方提供生成秘密令牌（自己持有）与访问令牌（可交予应用方），并注册可访问的服务域名白名单。
 
 - 将运行环境的域名作为系统参数，将安全中心发放的访问令牌作为ID，把秘密令牌（解密密钥）作为主密钥。
 
 - 认证的逻辑：用系统参数与ID对密钥密文进行加密，资源方使用主密钥解密。解密成功后，继续验证访问服务的域名是否在注册的白名单内，若是则通过认证。
2. **反调试**：反调试方案包括以下两点。
 - **强制debugger**：JavaScript自带debugger语法，可以检 



### 10.2.3 安全加固方案（续）
查调试面板是否处于打开状态，若是则进入无限调试状态。检查的时机选取有两种，利用定时器实时检查，或者在代码生成阶段随机在部分函数体中注入反调试函数来检查。
 
 - **过期时间**：对在线推理模块的整体运行时间和关键路径的运行时间进行约束，若超过设定的阈值，则触发反调试函数，强制结束运行，并上报异常。


3. **推理逻辑加固**：将整个推理运行时离线编译成WebAssembly模块（称为A模块），可保障代码安全。为了达到更高的安全性，还可采用加密方案中介绍的认证加密方式，首先把关键的推理逻辑代码编译成WebAssembly二进制格式，然后把它转换成ArrayBuffer再进行加密。在A模块中的身份认证与宿主校验环节进行的同时，对经过加密的推理逻辑代码进行解密并将其实例化成WebAssembly模块（称为B模块）。安全认证通过后，就可以调用B模块对模型进行推理。当然，这种加固方式会增加Web应用初始化的耗时，可在安全性和应用性两者间平衡选择。

#### 10.3 安全方案

模型内容安全、代码安全和安全加固方案的实现，需要安全中心、离线部署和在线推理三方配合完成，如图10-4所示，详细分工及合作方案如下。

**图10-4 安全方案的实现**
 - 离线部署对模型进行离线加密，生成加密后的模型文件（用于解密或推理的WASM文件 ），得到模型内容密文。
 - 安全中心生成access_key和secret_key，授权给应用方access_token和模型信息。
 - 在线推理在WASM运行时，进行解密或推理，包括宿主环境校验、时效性校验、解密、推理 。

![image](https://github.com/user-attachments/assets/1716d0db-e891-4491-ac7c-268e5474d7db)


##### 10.3.1 安全中心
安全中心是资源方与应用方之间的桥梁，资源方通过它进行备案和配置管理，应用方通过它进行授权和请求模型信息（认证服务）。

1. **备案**：资源方在安全中心备案后，会得到图10-4①步骤中的secret_key与access_key。其中secret_key由自己保存，不能交给他人，access_key可交给应用方。

2. **配置管理**：资源方对模型信息和授权访问限制进行配置。模型信息包括模型的两个网络文件地址和一些模型相关参数，这两个文件是由离线部署服务生成的，分别是加密后的模型文件和用于推理的运行时WebAssembly文件。授权访问限制的配置信息包含允许访问的服务域名白名单、授权服务的限制请求次数和请求逾期限制等。上述配置信息均可更新，secret_key也可重置，但access_key不可修改。

3. **授权**：应用方在请求模型资源前，要得到资源方的授权，安全中心会提供这种授权服务。授权服务可采用OAuth 2.0协议授权机制，这是一种广泛使用的授权技术。图10-5抽象地描述了OAuth 2.0协议的执行流程，其中的授权服务和资源提供服务都可以由安全中心承担。

![image](https://github.com/user-attachments/assets/3f34894d-90b1-4087-a013-5d5235e298b6)


**图10-5 OAuth 2.0协议的执行流程**
 - 客户端向资源方请求授权码。
 - 资源方返回授权码。
 - 客户端携带授权码请求令牌。
 - 资源方返回令牌（access_token）。
 - 客户端携带令牌请求资源。
 - 资源方返回资源。

OAuth 2.0协议有四种授权许可类型，授权码（Authorization Code）模式、简化（Implicit）模式、密码凭证（Resource Owner Password Credentials）模式和客户端凭证（Client Credentials）模式。推荐使用授权码模式，它的优点是安全性相对更高，缺点是需要Web AI应用有相应的服务端，但一般的Web AI应用都有相应的服务端。

图10-5中access_token的生成位于图10-4中的②步骤，可采用JWT（JSON Web Token，RFC 7519）技术生成Bearer Tokens（RFC 6750）。具体地，应用方的服务端携带资源方交予的access_key（由安全中心颁发），向安全中心的授权服务请求access_token，授权服务将请求服务的域名、时间戳等信息序列化后转换成一个JSON字符串，然后使用Base64进行编码，并拼接access_key，最后对这个字符串进行RSA加密签名，得到一个JWT，作为OAuth2.0中的access_token返回给应用方的服务端。
4. **认证服务**：在图10-4的③步骤中，认证服务会通过access_token的有效性来辨别应用方的身份。应用方的服务端收到客户端发起的获取模型信息的请求后，携带从授权服务中获取到的access_token向安全中心的认证服务发起请求。认证服务对接收到的access_token进行解密，对解密后得到的access_key、域名信息和时间逾期进行验证，验证无误后返回模型信息，应用方的服务端接收到后再返回给客户端。

至此，应用方的客户端收到模型信息，就可以进行后续的在线认证、解密、推理了。在这之前，先看一下离线部署是如何保障模型文件的安全性的。

##### 10.3.2 离线部署

要想对模型内容进行加解密，需要对模型内容进行预处理、生成模型密文，同时生成对应的解密模块。这个过程可被封装成一个SDK，供资源方进行离线部署。

1. **模型内容预处理**：


前端推理引擎在初始化阶段，首先会整合模型信息，生成神经网络拓扑结构，然后对图进行遍历，完成神经网络每一层算子的运算。所以在离线阶段，预先执行初始化，就能够获得经过优化、过滤的神经网络拓扑结构信息。进一步，还可以对其中会暴露模型结构特征的信息进行混淆处理，如算子名称、Tensor名称等，混淆方式可选择变量替换或转换成十六进制。至此，就得到了经过提取、过滤、混淆后的模型信息作为明文。接下来就是生成模型密文和解密模块。

4. **生成模型密文**：


对模型信息序列化后转换成二进制格式并加密，加密算法可选择混合密码体制或轻量的认证加密方案。加密后会生成模型内容的密文；用于解密的密钥也会被加密，生成密钥密文，由资源方保管；模型内容密文会通过哈希计算生成对应的认证码。认证码可插入模型内容的密文中，生成最终的模型文件。

6. **生成有解密功能的推理运行时WebAssembly模块**：


WebAssembly模块由安全加固、解密和推理三部分组成，其中安全加固的内容会在线推理环节进行详细介绍，推理的部分由前端推理引擎的WebAssembly计算方案实现。至于解密，在选定加密方案后，对应的解密方案也就确定了，将解密逻辑封装并编译到WebAssembly模块中。由于不同的产品需求适合的加密方式不同，因此可按照加密强度与解密速度衡量选择，只是加密方式一定要与推理运行时中的解密方式对应。

至此，就得到了模型密文文件与即将在线运行的WebAssembly文件。将两个文件的网络地址及模型相关信息配置到安全中心后，应用方就能够在通过授权的前提下请求到模型信息。

虽然模型密文文件与在线推理运行时WebAssembly文件是离线生成的，但生成速度很快，若模型要求安全性比较高，则资源方可提供在线服务实时生成这两个文件。例如，安全中心在收到来自应用方的模型信息请求时，将调用在线生成加解密文件的服务。

##### 10.3.3 在线推理
在线推理的核心能力是在离线私有化部署中生成的WebAssembly模块中完成的，所以Web AI应用要完成如图10-4所示的④步骤中的几项操作。

第一，授权以获取模型信息，授权的详细过程可回顾10.3.1节内容，大致流程如下。
1. 客户端向服务端发起获取模型信息的请求。
2. 服务端向安全中心请求获取模型信息的access_token，并将access_token缓存下来，定时更新，以免频繁请求被拒。
3. 服务端携带access_token向安全中心请求认证，认证通过后得到模型信息，交给客户端。
4. 客户端获取模型信息后，加载模型密文文件与WebAssembly文件并实例化WebAssembly，以获取推理运行时的入口模块。

第二，调用推理运行时入口，传入模型密文，完成安全加固、解密、推理和推理后处理。
1. **安全加固**：通过身份认证与宿主校验、反调试、推理逻辑加固等方式验证用户身份、运行环境的安全性与完整性，详细方案可参考10.2.3节内容。
2. **解密**：解密前要根据选定的加密方案获取密钥密文的解密密钥。例如，如果选定的加密方案是混合密码体制，且由资源方保存解密密钥，那么资源方需要提供相应的TLS服务，在Web应用运行时安全认证通过后提供解密密钥，Web应用客户端用它来对密钥密文进行解密，进而解密模型密文。
3. **推理**：拿到解密后的模型信息后，可通过WebAssembly计算方案进行推理。
4. **推理后处理**：推理完成后，如果推理结果需要保密，则可为推理结果选择合适的加密算法进行加密，可将解密过程与推理后处理过程编译到一个单独的WebAssembly模块中。

#### 10.4 总结
通过分析Web AI应用在Web环境下运行的整个过程，定位到存在安全隐患的薄弱环节，本章设计了一套方案来系统地解决Web AI推理及应用中的安全问题，保障资源方的模型安全性、完整性与可控性，防止应用方的应用被盗用、被调试。

首先，最好有一个安全中心用于隔离资源方与应用方，完成模型的配置管理与授权工作，保证安全的同时降低资源方的成本。

然后，在离线部署阶段，整合、过滤、混淆前端推理引擎的初始化后生成的模型拓扑结构信息，将结果作为待加密的明文信息，选择一种模型加密方案对模型明文信息进行加密，推荐使用混合密码体制或轻量的认证加密方案。接下来，将相应的解密算法集成到推理运行时中，编译成WebAssembly文件，作为在线推理的核心运行模块。

最后，当在线运行Web AI应用时，资源方向安全中心发送授权请求及认证服务请求，以获取模型信息。加载模型密文文件与WebAssembly文件并实例化WebAssembly模块，以获取推理运行时入口。调用推理运行时入口模块，并传入模型密文，以进行安全加固、解密、推理和推理后处理，推理结果按需加密。若需要保障推理的前后数据处理及后续的应用实现环节的安全性，则要将相应的逻辑连同选择的安全加固手段一起编译成WebAssembly文件。 
